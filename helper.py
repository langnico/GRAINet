# coding=utf-8
import os
import numpy as np
import argparse
import json
import codecs
from glob import glob
from keras import losses
from skimage.transform import resize

import preprocessing as prepro
from loss_functions import mean_IOU, emd, reverseKL, JSD, mse_weighted, mae_weighted

# set up printing values from numpy arrays to not use exponential representation
np.set_printoptions(suppress=True)


def str2bool(v):
    """
    Helper function to read input arguments as boolean.
    :param v: string
    :return: bool
    """
    if isinstance(v, bool):
       return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def setup_parser():
    """
    Setup an argparse object to parse input arguments.
    Default values are set to run the random cross-validation with 10 folds to regress a scalar output (mean diameter).
    :return: ArgumentParser object
    """
    parser = argparse.ArgumentParser()

    parser.add_argument("--verbose", default=1, help='0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.', type=int)

    parser.add_argument("--experiment_type", default='randCV', help="str: type of experiment", choices=['randCV', 'bankCV'])
    parser.add_argument("--data_npz_path", default=None, help="str: path to npz data file")
    parser.add_argument("--bank_names_path", default=None, help="str: path to txt file with bank names need for bankCV")
    parser.add_argument("--randCV_indices_path", default=None, help="str: path to npy file with list of fold indices for random crossval")
    parser.add_argument("--experiment_dir", default=None, help="str: output path to experiment directory")

    # inference
    parser.add_argument("--image_path", default=None, help="str: path full ortho image tif for inference")
    parser.add_argument("--inference_path", default=None, help="str: out path for inference")

    parser.add_argument("--img_rows", default=500, help="int: image rows", type=int)
    parser.add_argument("--img_cols", default=200, help="int: image columns", type=int)
    parser.add_argument("--channels", default=3, help="int: image channels", type=int)
    parser.add_argument("--bins", default=21, help="int: number of histogram bins", type=int)

    parser.add_argument("--batch_size", default=8, help="int: batch size", type=int)
    parser.add_argument("--nb_epoch", default=100, help="int: batch size", type=int)
    parser.add_argument("--base_lr", default=0.0003, help="float: base learning rate", type=float)
    parser.add_argument("--loss_key", default='mse', help="str: name of the loss that is optimized", choices=['kld', 'emd', 'mse', 'mae', 'iou', 'rkl', 'jsd', 'msew', 'maew'])

    parser.add_argument("--test_fold_index", default=0, help='subset (fold) index used for testing', type=int)
    parser.add_argument("--test_bank_name", default=None, help="str: name of test bank")

    parser.add_argument("--volume_weighted", type=str2bool, nargs='?', const=True, default=False, help="Bool: Train/test on volume weighted pdfs.")
    parser.add_argument("--output_dm", type=str2bool, nargs='?', const=True, default=True, help="Bool: Network outputs dm instead of distribution.")
    parser.add_argument("--downsample_factor", default=1., help="float: downsample factor. factor=2 reduces the image width and height by factor 2 (rounding down)", type=float)

    return parser


def get_loss_dict():
    """
    Definition of loss functions (evaluation metrics) used in this project.
    All losses expect two inputs (y_true, y_pred), i.e., ground truth and predictions.
    :return: dictionary with loss functions
    """
    loss_dict = {'kld': losses.kullback_leibler_divergence,
                 'rkl': reverseKL,
                 'jsd': JSD,
                 'iou': mean_IOU,
                 'mse': 'mean_squared_error',
                 'mae': 'mean_absolute_error',
                 'emd': emd,
                 'msew': mse_weighted,
                 'maew': mae_weighted}
    return loss_dict


def transform_histogram(cdf_all):
    """
    Transforms all ground truth given as cumulative distribution function (CDF) to probability density function (PDF).
    :param cdf_all: list of cdf arrays
    :return: np array containing pdf ground truth
    """
    pdf_list = []
    for cdf in cdf_all:
        pdf_list.append(cdf2pdf(cdf))
    pdf_arrays = np.array(pdf_list, dtype=np.float32)
    return pdf_arrays


def cdf2pdf(cdf):
    """
    Transform CDF to PDF.
    :param cdf: array
    :return: pdf with num_bins = len(cdf) - 1
    """
    pdf = np.diff(cdf)
    return np.array(pdf, dtype=np.float32)


def save_history(path, history):
    """
    Save keras training history as json.
    :param path: (string) output file path .json
    :param history: keras history output
    """
    new_hist = {}
    for key in list(history.history.keys()):
        if type(history.history[key]) == np.ndarray:
            new_hist[key] == history.history[key].tolist()
        elif type(history.history[key]) == list:
            if type(history.history[key][0]) == np.float64:
                new_hist[key] = list(map(float, history.history[key]))

    with codecs.open(path, 'w', encoding='utf-8') as f:
        json.dump(new_hist, f, separators=(',', ':'), sort_keys=True, indent=4)


def load_history(path):
    """
    Load saved keras history from json file.
    :param path: (string) output file path .json
    :return: history as dictionary
    """
    with codecs.open(path, 'r', encoding='utf-8') as f:
        n = json.loads(f.read())
    return n


def cum_histo(hist):
    """
    Create a cumulative histogram. (pdf to cdf)
    :param hist: array (histogram, pdf)
    :return: array (cdf)
    """
    hist_cum = np.cumsum(hist)
    return hist_cum


def save_histograms(file_name, predicted_histo, names):
    """
    Save all the predicted histograms in the excel file
    :param file_name: (string) output file path
    :param predicted_histo: array
    :param names: tile names
    """
    # check if histograms have length 22
    predicted_histo = [np.concatenate((np.array([0]), histo)) for histo in predicted_histo if len(histo) == 21]

    names = np.expand_dims(names, axis=1)
    data_to_save = np.hstack((names, predicted_histo))

    labels = "Name,d: 0.00 m,d: 0.01 m,d: 0.02m,d: 0.03 m,d: 0.04 m,d: 0.06 m,d: 0.08 m,d: 0.10 m,d: 0.12 m,d: 0.15 m,d: 0.20 m,d: 0.25 m,d: 0.30 m,d: 0.35 m,d: 0.40 m,d: 0.50 m,d: 0.60 m,d: 0.80 m,d: 1.0 m,d: 1.2 m,d: 1.5 m,d: 2.0 m"

    format_specifier = ['%f'] * 22
    format_specifier.insert(0, '%s')

    np.savetxt(file_name, data_to_save, fmt=format_specifier, delimiter=",", header=labels)


def save_dm(file_name, predicted_dm, names):
    """
    Save all the mean diameters in the excel file.
    :param file_name: (string) output file path
    :param predicted_dm: array
    :param names: tile names
    """
    data_to_save = np.stack((names, predicted_dm), axis=-1)

    labels = "Name,dm [cm]"
    format_specifier = ['%s', '%f']

    np.savetxt(file_name, data_to_save, fmt=format_specifier, delimiter=",", header=labels)


def get_relative_volume_distribution(relative_frequency_hist):
    """
    Transform frequency distribution to volume distribution.
    Each bin frequency is weighted by a volume proxy. Here this is the bin center grain size squared.
    :param relative_frequency_hist: array (pdf)
    :return: array (volume weighted pdf)
    """

    # upper limits of all classes
    edges = np.array(
        [0.0, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.50, 0.60, 0.80,
         1.0, 1.2, 1.5, 2.0], dtype=float)

    n_bins = len(edges) - 1
    # mean diameter of the class i
    dmi = np.zeros(n_bins)
    for d in range(0, n_bins):
        dmi[d] = (edges[d+1] + edges[d]) / 2.

    # percentage mass of class i, before additional empirical corrections
    mean_bin_volume = np.power(dmi, 2)
    volume_hist = relative_frequency_hist * mean_bin_volume
    relative_volume_hist = volume_hist / np.sum(volume_hist)

    return relative_volume_hist


def relFreq2relVolume(histograms):
    """
    Wrapper to transform all frequency distributions to volume distributions.
    :param histograms: array
    :return: array
    """
    rel_vol_hist_all = []
    for hist in histograms:
        rel_vol_hist_all.append(get_relative_volume_distribution(hist))
    return np.array(rel_vol_hist_all)


# calculate a mean diameters of a histogram
def get_dm(delta_qi, volume_weighted=False):
    """
    Calculate the mean diameters from pdf (either frequency or already volume weighted).
    We compute the mean diameter according to Fehr (1987).

    Fehr, R. (1987). Einfache Bestimmung der KorngrÃ¶ssenverteilung von Geschiebematerial
    mit Hilfe der Linienzahlanalyse. Schweizer Ingenieur und Architekt, 38(87), 1104-1109.

    :param delta_qi: array (pdf frequency or volume)
    :param volume_weighted: bool (if true, the input delta_qi the volume weighting is skipped)
    :return: float (mean diameter in cm)
    """

    if len(delta_qi) == 21:
        # ad a zero bin
        delta_qi = np.concatenate((np.array([0]), delta_qi))

    # delta_qi is a traditional histogram
    delta_qi = np.expand_dims(delta_qi, axis=1)

    # upper limits of all classes
    d_grenz = np.array(
        [0.00, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.50, 0.60, 0.80,
         1.0, 1.2, 1.5, 2.0], dtype=float)
    anz_intervall = len(d_grenz)

    # mean diameter of the class i
    dmi = np.zeros([anz_intervall, 1])

    for d in range(anz_intervall):
        if d == 0:
            dmi[d] = d_grenz[d] / 2
        else:
            dmi[d] = (d_grenz[d] + d_grenz[d - 1]) / 2

    if not volume_weighted:
        # percentage mass of class i, before additional empirical corrections
        delta_qi_dmi2 = delta_qi * np.power(dmi, 2)
        delta_pi = delta_qi_dmi2 / np.sum(delta_qi_dmi2)
    else:
        # input histogram is already volume weighted
        delta_pi = delta_qi

    # cumulative percentage of mass of class i, before additional empirical corrections
    pi = np.zeros([anz_intervall])

    for d in range(anz_intervall):
        if d == 0:
            pi[d] = 0
        else:
            pi[d] = pi[d - 1] + delta_pi[d]

    # cumulative percentage mass of class i, first corection: admitting 25% of the stones are smaller and not measured
    pi_c = np.zeros([anz_intervall, 1])

    for d in range(anz_intervall):
        if d == 0:
            pi_c[d] = 0
        else:
            pi_c[d] = 0.25 + 0.75 * pi[d]

    # Second correction - calculating an empirical curve, the so called Fuller-Curve
    pFU_1 = np.zeros([anz_intervall - 1, 1])
    diff_p = np.zeros([anz_intervall - 1, 1])

    diff_vg = 100

    # added new line
    rel_ind = 0

    for d in range(anz_intervall - 1):
        if d == 0:
            pass
        else:
            pFU_1[d] = np.sqrt(d_grenz[d + 1] / (d_grenz[d] / np.power(pi_c[d], 2)))
            diff_p[d] = np.abs(pi_c[d] - pFU_1[d])
            if pi_c[d] > 0:
                if pi_c[d] < 0.99999999999999:
                    if diff_p[d] < diff_vg:
                        rel_ind = d
                        diff_vg = diff_p[d]
    if rel_ind < 2:
        rel_ind = 2
    elif rel_ind > 7:
        rel_ind = 7

    piFU = np.zeros([anz_intervall, 1])

    for d in range(anz_intervall):
        piFU[d] = np.sqrt(d_grenz[d] / (d_grenz[rel_ind - 1] / np.power(pi_c[rel_ind - 1], 2)))

    pi_rel = np.zeros([anz_intervall, 1])

    pi_rel[0:rel_ind] = piFU[0:rel_ind]
    pi_rel[rel_ind::] = pi_c[rel_ind::]

    # index_1 = np.where(pi_rel > 0.99)[0][0] + 1
    delta_pi_rel = np.zeros([anz_intervall, 1])
    # zaehler = 0
    for d in range(anz_intervall - 1):
        if d == 0:
            delta_pi_rel[d] = pi_rel[d] - 0
        else:
            delta_pi_rel[d] = pi_rel[d] - pi_rel[d - 1]

    dm_t = np.multiply(delta_pi_rel, dmi)
    dm = np.sum(dm_t)

    # returning median diameter in cm
    return dm * 100


def make_crossval_ramdom_train_val_test_split(data, indices_list_path, test_fold):
    """
    Split data into random subsets using predefined sample indices.
    :param data: dictionary containing arrays
    :param indices_list_path: npy file path with stored fold sample indices
    :param test_fold: test fold index
    :return: dictionary
    """
    # load list of fold indices
    indices_list = list(np.load(indices_list_path, allow_pickle=True))

    test_indices = indices_list[test_fold]

    # all except indices from fold test fold  i
    trainval_indices = indices_list[:test_fold] + indices_list[test_fold+1:]
    #print('len training indices list: ', len(trainval_indices))
    trainval_indices = np.concatenate(trainval_indices)
    if not set(test_indices).isdisjoint(set(trainval_indices)):
        raise ValueError("TEST indices overlap with TRAINING indices.")

    # split training into 80% train and 20% val
    train_indices = trainval_indices[:int(0.9 * len(trainval_indices))]
    val_indices = trainval_indices[int(0.9 * len(trainval_indices)):]

    fold_data = {}

    # transform the histogram from the original cumulative to the traditional one
    histogram_all_transformed = transform_histogram(data['histograms'])

    # images
    fold_data['X_train'] = data['images'][train_indices]
    fold_data['X_val'] = data['images'][val_indices]
    fold_data['X_test'] = data['images'][test_indices]

    # labels
    fold_data['Y_train'] = histogram_all_transformed[train_indices]
    fold_data['Y_val'] = histogram_all_transformed[val_indices]
    fold_data['Y_test'] = histogram_all_transformed[test_indices]

    # names of tiles
    fold_data['N_train'] = data['tile_names'][train_indices]
    fold_data['N_val'] = data['tile_names'][val_indices]
    fold_data['N_test'] = data['tile_names'][test_indices]

    # dm
    fold_data['D_train'] = data['dm'][train_indices]
    fold_data['D_val'] = data['dm'][val_indices]
    fold_data['D_test'] = data['dm'][test_indices]

    fold_data['MEAN_train'], fold_data['STD_train'] = prepro.get_mean_and_std_per_channel(fold_data['X_train'])

    return fold_data


def is_tile_from_bank(tile_name, bank_name):
    """
    Check if image tile comes from particular river bank by name.
    :param tile_name: string
    :param bank_name: string
    :return: bool (True if tile comes from bank)
    """
    return tile_name[:-12] == bank_name


def make_crossval_banks_train_val_test_split(data, test_bank_name):
    """
    Split data into train, val, test, holding out a complete river bank for testing (by name).
    Used for generalization experiments.
    :param data: dictionary
    :param test_bank_name: string (river bank to test on)
    :return: dictionary
    """

    vec_is_tile_from_bank = np.vectorize(is_tile_from_bank)

    test_indices = np.squeeze(np.argwhere(vec_is_tile_from_bank(data['tile_names'], test_bank_name)))
    print('{}: {} test samples'.format(test_bank_name, test_indices.shape))

    if len(test_indices) == 0:
        raise ValueError('no test samples found, check test_bank_name: {}'.format(test_bank_name))

    # all except indices from fold test fold  i
    trainval_indices = np.argwhere(~vec_is_tile_from_bank(data['tile_names'], test_bank_name))
    print('trainval_indices.shape', trainval_indices.shape)
    trainval_indices = np.concatenate(trainval_indices)
    print('trainval_indices.shape', trainval_indices.shape)

    if not set(list(test_indices)).isdisjoint(set(list(trainval_indices))):
        raise ValueError("TEST indices overlap with TRAINING indices.")

    # split training into 80% train and 20% val
    train_indices = trainval_indices[:int(0.9 * len(trainval_indices))]
    val_indices = trainval_indices[int(0.9 * len(trainval_indices)):]

    fold_data = {}

    # transform the histogram from the original cumulative to the traditional one
    histogram_all_transformed = transform_histogram(data['histograms'])

    # images
    fold_data['X_train'] = data['images'][train_indices]
    fold_data['X_val'] = data['images'][val_indices]
    fold_data['X_test'] = data['images'][test_indices]

    # labels
    fold_data['Y_train'] = histogram_all_transformed[train_indices]
    fold_data['Y_val'] = histogram_all_transformed[val_indices]
    fold_data['Y_test'] = histogram_all_transformed[test_indices]

    # names of tiles
    fold_data['N_train'] = data['tile_names'][train_indices]
    fold_data['N_val'] = data['tile_names'][val_indices]
    fold_data['N_test'] = data['tile_names'][test_indices]

    # dm
    fold_data['D_train'] = data['dm'][train_indices]
    fold_data['D_val'] = data['dm'][val_indices]
    fold_data['D_test'] = data['dm'][test_indices]

    fold_data['MEAN_train'], fold_data['STD_train'] = prepro.get_mean_and_std_per_channel(fold_data['X_train'])

    print('train samples: ', len(fold_data['X_train']))
    print('val   samples: ', len(fold_data['X_val']))
    print('test  samples: ', len(fold_data['X_test']))

    return fold_data


def downsample_images(images, factor):
    """
    Reduce image resolution by factor (>1).
    :param images: array
    :param factor: float (e.g. factor=2 doubles the ground sampling distance.)
    :return: array (downsampled images)
    """
    images_out = []
    for i in range(len(images)):
        image = images[i]
        image_resized = resize(image, (image.shape[0] // factor, image.shape[1] // factor), anti_aliasing=True)
        images_out.append(image_resized)
        
    return np.array(images_out)


def create_k_fold_split_indices(data, out_path, num_folds=10):
    np.random.seed(21)
    # shuffle the images randomly to create train, val and test sets
    indices = np.arange(data['images'].shape[0])
    # always generate the same random numbers with random seed for testing the code
    np.random.shuffle(indices)
    # split indices int n folds:
    indices_list = np.array_split(indices, num_folds)
    # indices_list is saved as array of type object, because not all folds have the exact same length.
    np.save(out_path, indices_list)
    return np.array(indices_list, dtype=object)


def wrapper_make_data_split(experiment_type, data_npz_path, test_bank_name, test_fold_index, randCV_indices_path,
                            volume_weighted=False,
                            output_dm=False,
                            downsample_factor=1.):    
    """
    Wrapper to create train, val, test splits.
    Either random cross-validation (randCV) or bank cross-validation (bankCV).
    :param experiment_type: string (choices=['randCV', 'bankCV']
    :param data_npz_path: path to data
    :param test_bank_name: string (bank to test if experiment_type='bankCV')
    :param test_fold_index: int (fold index to test if experiment_type='randCV')
    :param randCV_indices_path: path to saved data split with sample indices
    :param volume_weighted: bool (True, if targets are volume weighted histograms)
    :param output_dm: bool (True, if targets are scalars, i.e. mean diameters)
    :param downsample_factor: float (if input image should be artificially downsampled)
    :return: dictionary
    """
    
    data_all = np.load(data_npz_path, allow_pickle=True)

    if experiment_type == 'randCV':
        data = make_crossval_ramdom_train_val_test_split(data=data_all, indices_list_path=randCV_indices_path, test_fold=test_fold_index)

    elif experiment_type == 'bankCV':
        data = make_crossval_banks_train_val_test_split(data=data_all, test_bank_name=test_bank_name)

    else:
        raise ValueError("experiment type: '{}' is not defined.".format(experiment_type))

    if volume_weighted:
        # labels
        print('converting labels to relative volume distributions (volume pdf)...')
        data['Y_train'] = relFreq2relVolume(data['Y_train'])
        data['Y_val'] = relFreq2relVolume(data['Y_val'])
        data['Y_test'] = relFreq2relVolume(data['Y_test'])

    if output_dm:
        # output dm instead of histogram
        data['Y_train'] = data['D_train']
        data['Y_val'] = data['D_val']
        data['Y_test'] = data['D_test']

    # For testing the effect of different input image resolutions
    if downsample_factor != 1:
        data['X_train'] = downsample_images(data['X_train'], factor=downsample_factor)
        data['X_val'] = downsample_images(data['X_val'], factor=downsample_factor)
        data['X_test'] = downsample_images(data['X_test'], factor=downsample_factor)

    return data


class NumpyEncoder(json.JSONEncoder):
    """
    Convert nparrays to lists for json writing
    """
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)


def collect_cv_data(parent_dir, loss_keys=('emd', 'iou', 'kld', 'mae', 'mse', 'maew', 'msew', 'rkl', 'jsd')):
    """
    Collect cross-validation outputs from all test folds.
    :param parent_dir: path to directory containing all fold directories.
    :param loss_keys: list of loss/metric keys to evaluate
    :return results_dict: evaluation metrics for all folds
    :return avg_results_dict: mean and std of metrics over all folds
    :return dm_results_dict: true and predicted mean diameters
    """
    loss_keys = sorted(loss_keys)

    results_dict = {}
    out_dir = os.path.join(parent_dir, 'collected')

    dm_results_dict = {}

    for loss_key in loss_keys:
        print('loss: ', loss_key)
        results_dict[loss_key] = {}
        dm_results_dict[loss_key] = {}
        predicted_dm_all = np.array([])
        true_dm_all = np.array([])

        # get all fold directories
        fold_dirs = glob(os.path.join(parent_dir, 'loss_{}'.format(loss_key), 'testfold_*'))
        print('Number of fold directories:', len(fold_dirs))
        # get all results keys and initialize lists
        tmp_dict = json.load(open(fold_dirs[0] + '/test_metrics.json', 'r'))
        for k in tmp_dict.keys():
            results_dict[loss_key][k] = []
        tmp_dict = json.load(open(fold_dirs[0] + '/test_dm_metrics.json', 'r'))
        for k in tmp_dict.keys():
            results_dict[loss_key][k] = []

        for fdir in fold_dirs:
            test_metrics = json.load(open(fdir + '/test_metrics.json', 'r'))
            for k in test_metrics.keys():
                results_dict[loss_key][k].append(test_metrics[k])
            test_dm_metrics = json.load(open(fdir + '/test_dm_metrics.json', 'r'))
            for k in test_dm_metrics.keys():
                results_dict[loss_key][k].append(test_dm_metrics[k])

            # collect dm results
            pred_dm = np.load(os.path.join(fdir, 'dm_pred.npy'))
            true_dm = np.load(os.path.join(fdir, 'dm_true.npy'))

            predicted_dm_all = np.concatenate((predicted_dm_all, pred_dm))
            true_dm_all= np.concatenate((true_dm_all, true_dm))

        dm_results_dict[loss_key]['dm_true'] = true_dm_all
        dm_results_dict[loss_key]['dm_pred'] = predicted_dm_all

    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    json.dump(results_dict, open(os.path.join(out_dir, 'results_dict.json'), 'w'))


    # average/ std over folds
    avg_results_dict = {}
    for loss_key in loss_keys:
        avg_results_dict[loss_key] = {}
        for metric in results_dict[loss_key].keys():
            avg_results_dict[loss_key][metric] = {}

    for loss_key in loss_keys:
        for metric in avg_results_dict[loss_key].keys():
            # convert lists to numpy
            results_dict[loss_key][metric] = np.array(results_dict[loss_key][metric], dtype=np.float32)
            # reduce arrays to mean and std
            avg_results_dict[loss_key][metric]['mean'] = float(np.mean(results_dict[loss_key][metric]))
            avg_results_dict[loss_key][metric]['std'] = float(np.std(results_dict[loss_key][metric]))
    json.dump(avg_results_dict, open(os.path.join(out_dir, 'avg_results_dict.json'), 'w'))
    json.dump(dm_results_dict, open(os.path.join(out_dir, 'dm_results_dict.json'), 'w'), cls=NumpyEncoder)

    return results_dict, avg_results_dict, dm_results_dict


# per table row: create a list of strings
def dict_to_table(avg_results_dict, headers, losses=None):
    table = []
    if losses is None: 
        losses = headers
        
    for loss in losses:
        row_mean_list = []
        row_std_list = []
        row_mean_list.append(loss.replace('_', ' '))
        row_std_list.append('')
        for metric in headers:
            row_mean_list.append("${:.2f}$".format(avg_results_dict[loss][metric]['mean']))
            row_std_list.append("$({:.2f})$".format(avg_results_dict[loss][metric]['std']))
        table.append(row_mean_list)
        table.append(row_std_list)
    return table


